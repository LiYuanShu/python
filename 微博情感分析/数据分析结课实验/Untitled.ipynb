{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    啊呀呀 ！ 要死 啦 ！ 么 么 么 ！ 只 穿 外套 就 好 了 ， 我 认为 里面 那 ...\n",
      "1                         风格 不 一样 嘛 ， 都 喜欢 ！ 最 喜欢 哪张 ？\n",
      "2    好 呀 ， 试试 D . I . Y . 去死皮 面膜 1 . 将 燕麦片 加水 中 浸泡 ...\n",
      "3    张 1 老师 ， 谢谢 侬 的 1 信任 ！ 粉丝 多少 无所谓 重在 质地 近日 发现 一...\n",
      "4    第二 條看 來 有點 吸引力 呵呵 【 美国 相亲 节目 与 中国 的 1 几大 不同 】 ...\n",
      "Name: cut_text, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "#把停用词转换为一个列表存储\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "df = pd.read_csv('test.csv')\n",
    "X = df[['text']]\n",
    "y = df.type\n",
    "X['cut_text'] = X.text.apply(chinese_word_cut)\n",
    "print(X.cut_text[:5])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.01, random_state=1)\n",
    "stop_words_file = 'F:\\python\\数据分析结课实验\\data/停用词.txt'\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "# 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "max_df = 0.8\n",
    "# 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "min_df = 4 \n",
    "vect = CountVectorizer(max_df = max_df,min_df = min_df,token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',stop_words=frozenset(stopwords))\n",
    "term_matrix = pd.DataFrame(vect.fit_transform(X_train.cut_text[0:300]).toarray(), columns=vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 500\n",
      "500 1000\n",
      "1000 1500\n",
      "1500 2000\n",
      "2000 2500\n",
      "2500 3000\n",
      "3000 3500\n",
      "3500 4000\n",
      "4000 4500\n",
      "4500 5000\n",
      "5000 5500\n",
      "5500 6000\n",
      "6000 6500\n",
      "6500 7000\n",
      "7000 7500\n",
      "7500 8000\n",
      "8000 8500\n",
      "8500 9000\n",
      "9000 9500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9500, 42582)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "num_lists=[i for i in range(0,10000,500)]\n",
    "# num_lists.append(len(X_train.cut_text))\n",
    "\n",
    "frames=[]\n",
    "for i in range(len(num_lists)-1):\n",
    "    j=i+1\n",
    "    print(num_lists[i],num_lists[j])\n",
    "    term_matrix = pd.DataFrame(vect.fit_transform(X_train.cut_text[num_lists[i]:num_lists[j]]).toarray(), columns=vect.get_feature_names())\n",
    "    frames.append(term_matrix)\n",
    "result=pd.concat(frames) \n",
    "print(result.shape)\n",
    "# term_matrix1 = pd.DataFrame(vect.fit_transform(X_train.cut_text[0:200]).toarray(), columns=vect.get_feature_names())\n",
    "# term_matrix2 = pd.DataFrame(vect.fit_transform(X_train.cut_text[200:400]).toarray(), columns=vect.get_feature_names())\n",
    "# term_matrix.\n",
    "# term_matrix.append(term_matrix2)\n",
    "# print(len(X_train.cut_text))\n",
    "# print(term_matrix1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('countvectorizer',\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "          dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "          lowercase=True, max_df=0.8, max_features=None, min_df=4,\n",
       "          ngram_range=(1, 1), preprocessor=None,\n",
       "          stop_words=frozenset({'便于', '设或', '哗', '用', '”', '甚且', '临', '乘', '然则', '啐', '纵', '说来', '非但', '以及', '呀', '在', '为止', '这般', '那么些', '一旦', '不光', '咦', '哉', '哦', '你', '假如', '哟', '对', '不成', '总之', '据', '全体', '?', '固然', '哼', '而外', '对于', '别是', '、', '总的说来', '就算', '此时', '始而', '以故', '只是', '紧接着', '处在', '且', '非独', ...是说', '云云', '谁知', '咚', '哪年', '要不是', '再其次', '万一', '各位', '这儿', '接着', '那般', '别说', '多么', '此地', '诸', '都'}),\n",
       "          strip_accents=None, token_pattern='(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',\n",
       "          tokenizer=None, vocabulary=None)),\n",
       " ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "nb = MultinomialNB()\n",
    "pipe = make_pipeline(vect, nb)\n",
    "pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6198575913510876"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X_train.cut_text, y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=0.8, max_features=None, min_df=4,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=...e, vocabulary=None)), ('multinomialnb', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train.cut_text, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0, 0, 0, 2, 2, 0, 0, 2, 0, 0, 0, 2, 2, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       2, 3, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1, 0, 2, 2, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 2, 2,\n",
       "       0, 1, 3, 0, 0, 0, 3, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 2, 0, 2, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 2,\n",
       "       1, 1, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 2, 2, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 2, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0,\n",
       "       0, 0, 2, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 2, 0, 0,\n",
       "       0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 2, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0], dtype=int64)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.predict(X_test.cut_text)[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipe.predict(X_test.cut_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6219054763690923"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1341,  112,  109,   35],\n",
       "       [ 236,  123,   45,   18],\n",
       "       [ 203,   59,  169,   16],\n",
       "       [ 130,   23,   22,   25]], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9922      0.997301\n",
       "205908    0.303299\n",
       "164063    0.947852\n",
       "28882     0.999999\n",
       "202471    0.999923\n",
       "Name: cut_text, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "from interval import Interval\n",
    "def get_sentiment(text):\n",
    "    return SnowNLP(text).sentiments\n",
    "y_pred_snownlp = X_test.cut_text.apply(get_sentiment)\n",
    "y_pred_snownlp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-23-2498d2cefbbd>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-23-2498d2cefbbd>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    y_pred_snownlp_normalized = y_pred_snownlp.apply(lambda x: 0 if x in Interval(0, 1)  1 if x in Interval(1, 2, lower_closed=False)  2 if x in Interval(2, 3, lower_closed=False) else 3 )\u001b[0m\n\u001b[1;37m                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9922      0\n",
       "205908    1\n",
       "164063    0\n",
       "28882     0\n",
       "202471    0\n",
       "Name: cut_text, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dd(x):\n",
    "    if x in Interval(0,0.1):\n",
    "        return 3\n",
    "    if x in Interval(0.1, 0.3, lower_closed=False):\n",
    "        return 2\n",
    "    if x in Interval(0.3, 0.5, lower_closed=False):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    \n",
    "y_pred_snownlp_normalized = y_pred_snownlp.apply(dd)\n",
    "y_pred_snownlp[0:5]\n",
    "y_pred_snownlp_normalized[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1080,  118,  138,  261],\n",
       "       [ 222,   30,   40,  130],\n",
       "       [ 246,   36,   51,  114],\n",
       "       [ 107,   13,   21,   59]], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(y_test, y_pred_snownlp_normalized)\n",
    "metrics.confusion_matrix(y_test, y_pred_snownlp_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9922      0.997301\n",
       "205908    0.303299\n",
       "164063    0.947852\n",
       "28882     0.999999\n",
       "202471    0.999923\n",
       "Name: cut_text, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_snownlp[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#把记事本中的数据去掉词语的词性重新写入csv文件，并且标记好分类\n",
    "import re\n",
    "import pandas as pd\n",
    "def proc_text(text):\n",
    "    pos_words=[]\n",
    "    pos_list=[]\n",
    "    type_list = []\n",
    "    for file in text:\n",
    "        with open(file,encoding='UTF-8') as f:\n",
    "            line = f.readline()\n",
    "            while(line):\n",
    "                line = line.strip('\\n')\n",
    "                line=re.sub('/[a-zA-Z]+', '',line).replace(\" \",\"\")\n",
    "                pos_list.append(line)\n",
    "                type_list.append(int(file[7]))\n",
    "                line = f.readline()\n",
    "    dataframe = pd.DataFrame({'text': pos_list, 'type': type_list})\n",
    "    dataframe.to_csv(\"./data/data.csv\", index=False, sep=',',encoding=\"utf_8_sig\")\n",
    "texts_list =['./data/0_simplifyweibo.txt','./data/1_simplifyweibo.txt','./data/2_simplifyweibo.txt','./data/3_simplifyweibo.txt']\n",
    "proc_text(texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    啊呀呀 ！ 要死 啦 ！ 么 么 么 ！ 只 穿 外套 就 好 了 ， 我 认为 里面 那 ...\n",
      "1                         风格 不 一样 嘛 ， 都 喜欢 ！ 最 喜欢 哪张 ？\n",
      "2    好 呀 ， 试试 D . I . Y . 去死皮 面膜 1 . 将 燕麦片 加水 中 浸泡 ...\n",
      "3    张 1 老师 ， 谢谢 侬 的 1 信任 ！ 粉丝 多少 无所谓 重在 质地 近日 发现 一...\n",
      "4    第二 條看 來 有點 吸引力 呵呵 【 美国 相亲 节目 与 中国 的 1 几大 不同 】 ...\n",
      "5    喜欢 苹果 IPHONE4 。 功能强大 ， 时尚 ， 手机 功能 多 。 沃爱 平谷 第二...\n",
      "6    回复 太牛 了 买房子 送 瓷砖 呗 ！ 昨晚 上 经过 中润 ， 看到 的 1 一个 立柱...\n",
      "7    人们 脱口而出 的 1 一般 都 是 没有 实际意义 的 1 话 — — — — 噗 … …...\n",
      "8    开张大吉 ！ 祝贺 买卖 兴隆 我家 的 1 酸辣粉 小店 开业 了 欢迎 铁岭 的 1 朋...\n",
      "9    方 大水 你 怎么 可以 这么 萌 这么 有 爱 捏 啊 呜 ~ ~ 偶 不要 变成 HC ...\n",
      "Name: cut_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import jieba\n",
    "from gensim.models import word2vec\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "def chinese_word_cut(mytext):\n",
    "    return \" \".join(jieba.cut(mytext))\n",
    "#把停用词转换为一个列表存储\n",
    "def get_custom_stopwords(stop_words_file):\n",
    "    with open(stop_words_file) as f:\n",
    "        stopwords = f.read()\n",
    "    stopwords_list = stopwords.split('\\n')\n",
    "    custom_stopwords_list = [i for i in stopwords_list]\n",
    "    return custom_stopwords_list\n",
    "df = pd.read_csv('./data/data.csv')\n",
    "X = df[['text']]\n",
    "y = df.type\n",
    "#对评论语句进行分词\n",
    "X['cut_text'] = X.text.apply(chinese_word_cut)\n",
    "print(X['cut_text'][0:10])\n",
    "#分隔训练集和测试集，训练集为90%，测试集为10%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.1, random_state=0)\n",
    "stop_words_file = './data/停用词.txt'\n",
    "#停用词列表\n",
    "stopwords = get_custom_stopwords(stop_words_file)\n",
    "# 在超过这一比例的文档中出现的关键词（过于平凡），去除掉。\n",
    "max_df = 0.9\n",
    "# 在低于这一数量的文档中出现的关键词（过于独特），去除掉。\n",
    "min_df = 4 \n",
    "#特征提取，使用 CountVectorizer向量化工具，它依据词语出现频率转化向量。\n",
    "#特征提取使用“一袋子词”（bag of words）模型。一袋子词模型不考虑词语的出现顺序，\n",
    "#也不考虑词语和前后词语之间的连接。每个词都被当作一个独立的特征来看待。可能会因为没有联系上下文到达效果不如人意。\n",
    "vect = CountVectorizer(max_df = max_df,min_df = min_df,token_pattern=u'(?u)\\\\b[^\\\\d\\\\W]\\\\w+\\\\b',stop_words=frozenset(stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "#分类模型，采用朴素贝叶斯\n",
    "nb = MultinomialNB()\n",
    "#利用管道(pipeline)把vect 和 nb 串联起来\n",
    "pipe = make_pipeline(vect, nb)\n",
    "#查看模型的工作步骤\n",
    "#pipe.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6182463909309222"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#把训练集内容输入，做k折交叉验证，算出模型分类准确率的均值。\n",
    "#cv的值代表k，初始训练样本分成k份，其中（k-1）份被用作训练集，剩下一份被用作评估集，这样一共可以对分类器做k次训练，并且得到k个训练结果。\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(pipe, X_train.cut_text, y_train, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['若果'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 2 0 1 0 0 0 0 1 1 0 0 2 0 0 0 0 1 0 0 3 0 0 0 0 0 0 0 0 1 0 0 0 2 0 2\n",
      " 0 0 0 0 0 2 2 1 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#使用用训练集，把模型拟合出来\n",
    "pipe.fit(X_train.cut_text, y_train)\n",
    "#在测试集上，对情感分类标记进行预测。\n",
    "result=pipe.predict(X_test.cut_text)\n",
    "#查看测试集上前五十个的预测结果\n",
    "print(result[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对测试集的预测结果的正确率： 0.6287097137282858\n",
      "混淆矩阵：\n",
      " [[13504  1079  1047   386]\n",
      " [ 2097  1391   488   119]\n",
      " [ 2177   583  1590   126]\n",
      " [ 1279   301   214   272]]\n"
     ]
    }
   ],
   "source": [
    "#使用scikit-learn给我们提供的模型性能测度工具，帮助我们为预测结果和实际情况进行对比\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "TR = tree.DecisionTreeClassifier(criterion='entropy') \n",
    "corretcRate=metrics.accuracy_score(y_test,result)\n",
    "print(\"对测试集的预测结果的正确率：\",corretcRate)\n",
    "confusionMatrix=metrics.confusion_matrix(y_test, result)\n",
    "print(\"混淆矩阵：\\n\",confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "对测试集的预测结果的正确率： 0.5711552170487375\n",
      "混淆矩阵：\n",
      " [[12325  1330  1608   753]\n",
      " [ 2118  1129   626   222]\n",
      " [ 2253   572  1431   220]\n",
      " [ 1193   258   277   338]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import tree\n",
    "TR = tree.DecisionTreeClassifier(criterion='entropy') \n",
    "\n",
    "pipe = make_pipeline(vect, TR)\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "pipe.fit(X_train.cut_text, y_train)\n",
    "result=pipe.predict(X_test.cut_text)\n",
    "\n",
    "from sklearn import metrics\n",
    "corretcRate=metrics.accuracy_score(y_test,result)\n",
    "print(\"对测试集的预测结果的正确率：\",corretcRate)\n",
    "confusionMatrix=metrics.confusion_matrix(y_test, result)\n",
    "print(\"混淆矩阵：\\n\",confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.18194321 1.2179586  0.98896925 0.94771961 1.40936864 0.72496582\n",
      " 0.50504429 0.79771988 0.57017839 0.93126652]\n",
      "174595    1\n",
      "164613    1\n",
      "101359    0\n",
      "32595     0\n",
      "18903     0\n",
      "89051     0\n",
      "107332    0\n",
      "59751     0\n",
      "115417    0\n",
      "225813    2\n",
      "Name: type, dtype: int64\n",
      "混淆矩阵：\n",
      " [[1 0 0]\n",
      " [0 1 0]\n",
      " [0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(result[:10])\n",
    "print(y_test[:10])\n",
    "confusionMatrix=metrics.confusion_matrix([1,2,3], [1,2,3])\n",
    "print(\"混淆矩阵：\\n\",confusionMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-08181f34da7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cut_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
